---

title: "多模态大模型入门：从“看得懂”到“会行动”的AI"

date: 2026-01-19

tags: ["AI", "大模型", "多模态", "学习笔记"]

draft: false

---



\# 多模态大模型入门：从“看得懂”到“会行动”的AI



最近一年里，“多模态大模型（Multimodal LLM / MLLM）”越来越频繁地出现在论文、产品和新闻里。相比传统只能处理文本的语言模型，多模态大模型的目标更直接：\*\*让模型不仅能读文字，还能理解图片、语音、视频等多种信息，并把它们统一到同一套推理与生成能力中。\*\*



这篇文章会用比较直观的方式，介绍多模态大模型是什么、能做什么、核心技术路线是什么，以及它未来可能的走向。



---



\## 1. 什么是多模态大模型？



在 AI 里，“模态（Modality）”可以理解为信息的形式，例如：



\- 文本（Text）

\- 图片（Image）

\- 语音（Audio）

\- 视频（Video）

\- 传感器信号（Sensor / Time-series）

\- 3D 点云（Point Cloud）



\*\*多模态大模型\*\*就是能够同时理解和处理多种模态输入的模型，典型任务包括：



\- 看图回答问题（VQA）

\- 读图理解并总结（Image Caption / OCR + Reasoning）

\- 根据文字生成图片（Text-to-Image）

\- 根据图片生成文字或代码（Image-to-Text / Image-to-Code）

\- 语音对话（Speech-to-Text + LLM + Text-to-Speech）

\- 视频理解与视频问答（Video QA / Video Caption）

\- 具身智能（机器人“看+听+说+做”）



一句话总结：  

> 多模态大模型 = \*\*统一的“理解+推理+生成”引擎\*\*，输入不止文本。



---



\## 2. 为什么多模态大模型很重要？



如果只做文本模型，它能解决的事情其实有明显边界：世界的绝大多数信息不是以纯文字形式存在的。



多模态能力的意义在于：  

\- \*\*更接近人类获取信息的方式\*\*（看图、听声音、结合上下文理解）

\- \*\*更适合真实世界任务\*\*（比如拍照识别、文档理解、视频分析）

\- \*\*更容易落地成产品\*\*（拍照问答、AI 助手、搜索、客服、教育）



比如一个真实的使用场景：



> 你把一张电路图拍照发给模型，让它解释电路功能，并指出可能的设计问题。  

传统 LLM 做不到，多模态模型就能做“图像理解 + 逻辑推理 + 文本解释”。



---



\## 3. 多模态大模型到底“长什么样”？



从工程结构上看，一个典型多模态大模型可以拆成三部分：



1\. \*\*模态编码器（Encoder）\*\*：把图片/音频/视频转成向量表示  

2\. \*\*对齐模块（Alignment / Adapter）\*\*：把不同模态对齐到同一个语义空间  

3\. \*\*语言模型（LLM）\*\*：负责推理、对话、生成结果



你可以把它想象成：



> 图片/语音/视频 → “翻译成 LLM 能懂的语言” → LLM 进行推理 → 输出答案



---



\## 4. 多模态模型是怎么“看懂图片”的？



这里有一个非常核心的技术点：\*\*图片不是一句话，而是一个二维信号\*\*。



主流路线是先用视觉模型（如 ViT）把图片切成很多块（patch），然后得到一串视觉 token：



\- 一张图片 → 分成 N 个 patch  

\- 每个 patch → 变成一个向量  

\- 最终得到：`\[v1, v2, ..., vN]`



然后把这些视觉 token 接入 LLM，让 LLM 能“像读句子一样读图片”。



常见的接入方式包括：



\- \*\*直接拼接到输入序列\*\*（把视觉 token 当成特殊 token）

\- \*\*用一个投影层把维度对齐\*\*（Linear / MLP Projector）

\- \*\*用 Cross-Attention 融合\*\*（让文本去注意视觉信息）



---



\## 5. 训练多模态大模型通常分几步？



一个比较典型的训练流程是：



\### 5.1 预训练（Pretrain）

用大量图文对（image-text pairs）训练，让模型学会基础对齐能力，例如：



\- 给图片，预测描述文字

\- 给文字，学习对应图片的语义特征



这一步类似让模型学会“图片和文字之间的对应关系”。



\### 5.2 指令微调（Instruction Tuning）

让模型更像“助手”，能按人类指令完成任务，比如：



\- “请解释这张图的内容”

\- “请从图中提取关键信息并总结”

\- “这张图里有什么异常？”



这一步通常会用大量人工标注或合成数据（instruction data）。



\### 5.3 对齐与偏好优化（RLHF / DPO 等）

让输出更符合人类偏好：更安全、更准确、更有帮助。



---



\## 6. 多模态大模型能做哪些事情？



下面按能力分类总结一下（也是你写博客时很好用的结构）。



\### 6.1 图像理解（Image Understanding）

\- 看图描述（Caption）

\- 视觉问答（VQA）

\- OCR + 理解（文档、截图、试卷、论文图片）



\### 6.2 推理与数学（Reasoning）

\- 图表分析（折线图、柱状图、散点图）

\- 几何题、函数图像理解

\- 实验结果图的解释



\### 6.3 代码与工程（Image-to-Code）

\- UI 截图 → HTML/CSS/React 代码

\- 流程图 → 代码结构建议

\- 报错截图 → 定位问题并给出修复方案



\### 6.4 语音与视频（Audio/Video）

\- 语音对话助手

\- 视频内容总结

\- 视频问答（“这个视频里第 30 秒发生了什么？”）



---



\## 7. 多模态大模型的难点在哪里？



多模态并不是“把模型堆大”就行，它有一些天然困难：



\### 7.1 视觉信息密度太高

一张图的信息量远大于一句话，模型要学会关注重点，否则会“看了但没看懂”。



\### 7.2 幻觉（Hallucination）

模型可能会编造图片里不存在的内容，比如：

\- 明明没有文字，它说“图里写着 XXX”

\- 明明是蓝色，它说成红色



\### 7.3 数据与标注成本高

高质量的图文数据、视频数据非常贵，尤其是“推理型标注”。



\### 7.4 评估困难

多模态模型的好坏不只看 BLEU/ROUGE，还要看推理正确性、对齐能力、安全性等。



---



\## 8. 未来趋势：多模态 + Agent + 工具调用



我个人认为多模态的下一步不是“能看懂”，而是“能做事”。



未来的多模态系统会越来越像：



\- 多模态感知（看/听/读）

\- LLM 推理（想）

\- 工具调用（查、算、执行）

\- 输出行动结果（做）



比如：

\- 拍一张实验装置照片 → 模型识别仪器型号 → 自动生成实验记录模板  

\- 上传一份论文 PDF 截图 → 自动提取公式 → 推导并写成笔记  

\- 给 UI 设计图 → 自动生成前端页面并能运行



当“多模态理解”与“工具调用”结合后，它就不只是聊天机器人，而是一个真正的智能助理系统。



---



\## 9. 总结



多模态大模型的本质，是把“现实世界的信息”转换成模型可以推理的统一表示，然后通过大语言模型的能力输出答案或行动方案。



如果你把纯文本 LLM 看成“会说话的 AI”，那么多模态大模型更像是：



> \*\*会看、会听、会读图表，还能推理与执行的 AI。\*\*



后续我也打算继续写一些更具体的内容，比如：

\- 多模态模型的数据构造方式

\- 图像 token 的设计与压缩

\- 视觉推理的典型失败案例

\- 多模态 + RAG / 多模态检索系统



---



\## 参考阅读（可选）

\- Vision Transformer (ViT)

\- CLIP：图文对齐经典方法

\- LLaVA：视觉指令微调路线代表

\- BLIP / BLIP-2：图文生成与对齐

\- GPT-4V / Gemini 系列：多模态助手方向



（后面我会把这些内容拆成单独文章详细写）



